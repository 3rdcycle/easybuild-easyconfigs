The test fails on a node with more than 5 V100 GPUs or more than 4 A100 GPUs.
Hence limit the world_size to 4
See https://github.com/pytorch/pytorch/issues/78975

Author: Alexander Grund (TU Dresden)

diff --git a/test/distributed/fsdp/test_fsdp_pure_fp16.py b/test/distributed/fsdp/test_fsdp_pure_fp16.py
index eea03bea3d..d3a4bb8257 100644
--- a/test/distributed/fsdp/test_fsdp_pure_fp16.py
+++ b/test/distributed/fsdp/test_fsdp_pure_fp16.py
@@ -32,6 +32,11 @@ if TEST_WITH_DEV_DBG_ASAN:
 
 class TestPureFP16(FSDPTest):
 
+    @property
+    def world_size(self):
+        # Test fails due to inaccuracies when using more than 4 GPUs
+        return min(4, super().world_size)
+
     @skip_if_lt_x_gpu(2)
     @parametrize(
         "cpu_offload",
