name = 'NVHPC'
version = '20.7'
local_gccver = '9.3.0'
versionsuffix = '-GCC-%s' % local_gccver

homepage = 'https://developer.nvidia.com/hpc-sdk/'
description = """C, C++ and Fortran compilers included with the NVIDIA HPC SDK (previously: PGI)"""

toolchain = SYSTEM

# NVHPC can be downloaded freely from NVIDIA's website at https://developer.nvidia.com/hpc-sdk
# It requires accepting the HPC SDK Software License Agreement (https://docs.nvidia.com/hpc-sdk/eula/index.html)
# If you accept the License Agreement, you may also uncomment the following line to automatically download the sources
# source_urls = ['https://developer.download.nvidia.com/hpc-sdk/%(version)s/']
sources = ['nvhpc_2020_%(version_major)s%(version_minor)s_Linux_x86_64_cuda_multi.tar.gz']
checksums = ['a5c5c8726d2210f2310a852c6d6e03c9ef8c75e3643e9c94e24909f5e9c2ea7a']

dependencies = [
    ('GCCcore', local_gccver),
    ('binutils', '2.34', '', ('GCCcore', local_gccver)),
    ('CUDA', '11.0.2'),
    # This is necessary to avoid cases where just libnuma.so.1 is present in the system and -lnuma fails
    ('numactl', '2.0.13', '', ('GCCcore', local_gccver))
]

default_cuda_version = None  # Use strings like "11.0" or use the command line: --try-amend=default_cuda_version="10.2"

# NVHPC EasyBlock supports some features, which can be set via CLI or this easyconfig.
# The following list gives examples for the easyconfig
#
# Install NVHPC for an associated CUDA version:
#   default_cuda_version = "11.0"
# Can also be specified via --try-amend=default_cuda_version="10.2"
# If not given, is tried to be taken from the CUDA module, if the CUDA module is a dependency
#
# Define a NVHPC-default Compute Capability
#   cuda_compute_capabilities = "8.0"
# Can also be specified via --cuda-compute-capabilities=8.0
# Only single values supported, not lists of values!
#
# Options to add/remove things to/from environment module (defaults shown)
#   module_byo_compilers = False  # Remove compilers from PATH (Bring-your-own compilers)
#   module_nvhpc_own_mpi = False  # Add NVHPC's own pre-compiled OpenMPI
#   module_add_math_libs = False  # Add NVHPC's math libraries (which should be there from CUDA anyway)
#   module_add_profilers = False  # Add NVHPC's NVIDIA Profilers
#   module_add_nccl = False       # Add NVHPC's NCCL library
#   module_add_nvshmem = False    # Add NVHPC's NVSHMEM library
#   module_add_cuda = False       # Add NVHPC's bundled CUDA

# this bundle serves as a compiler-only toolchain, so it should be marked as compiler (important for HMNS)
moduleclass = 'compiler'